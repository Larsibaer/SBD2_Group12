---
title: "SBD2_Group12"
author: "Lars Wenger"
date: '2022'
output:
    html_document:
    df_print: paged
    toc: true
    theme: united
  HTML: default
  word_document: default
  pdf_document: default
---
````{r setup, include=FALSE, echo=FALSE}
# Getting started 
knitr::opts_chunk$set(echo = TRUE)

libraries = c("readr", "ggplot2","Boruta", "dlookr", "ROCR", "caret", "pROC", "dplyr", "ROSE", "corrplot", "DescTools", "ggpubr", "tidyverse", "RColorBrewer", "ggcorrplot", "PerformanceAnalytics", "corrr", "networkD3", "reshape", "knitr")
 
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})

lapply(libraries, library, quietly = TRUE, character.only = TRUE)

rm(list=ls())

set.seed(7)
setwd("C:/Users/Student/SBD2/SBD2_Group12")
data <- read_csv("loan_sample_12.csv")
data <- data.frame(data)
```

#Exercise 1
## Data description
### What are the dimensions of the data set?
For this we use the function str(). We see, that there are 40'000 observations of 17 variables.
```{r, echo=FALSE}
str(data)
```

### How many numeric and how many categorical variables are included in the data?
For this we use also the function str(). We see, that there are 12 numerical and 5 characterical variables.
```{r}
str(data)
```

### Summarize the variables. Discuss the summary statistics obtained.
To Summarize the variables we use the function summary(). Looking at the variables we can see that the mean of most variables is close to their median, indicating a low ocurrence of outliers. The only exception here is the variable "tot_cur_bal" which has a big difference between mean and median, indicating a higher occurence of outliers. The variable "Status" is also special, containing only values of 0 and 1.
```{r}
summary(data)
```

### Check the levels of the target variable by choosing the appropriate visualization. Is the target variable balanced?
In the next step, we investigate our target variable. We notice that in our sample, we have 34'813 companies which did not default on their loan and we have 5'187 which did default. 
```{r}
table(data$Status)
barplot(table(data$Status))
```
The target variable is not balanced, therefore we are going to undersample it. 
```{r}
set.seed(7)
data_balanced <- ovun.sample(Status ~ ., data=data, method = "under")
data_under <- data.frame(data_balanced[["data"]])

table(data_under$Status)
barplot(table(data_under$Status))
```

### Check the distribution of the numeric variables in the data set (include different visual representations).
```{r, echo=FALSE}
PercTable(data$Status)
```



##Investigate whether certain variables contain outliers (hint: what does a box plot show?). Elaborate your view on how to proceed in dealing with the outliers and – if necessary – take appropriate action.

```{r}
#Dimensions of new set
dim(data_under)

#Numeric Variables
data_under_num <- data_under %>%
  select_if(is.numeric)
data_under_num <- as.data.frame(data_under_num)
dim(data_under_num)


#Categorical Variables
data_under_cat <- data_under %>%
  select_if(is.character)
data_under_cat <- as.data.frame(data_under_cat)
dim(data_under_cat)

boxplot(scale(data_under_num), xaxt = "n")
text(x = 1:length(data_under_num),
     y = par("usr")[3] - 0.8,
     labels = names(data_under_num),
     xpd = NA,
     ## Rotate the labels by 35 degrees.
     srt = 35,
     cex = 0.8,
     adj = 1)

#Diagnose Outliers
diagnose_outlier(data_under_num)
diagnose_numeric(data_under)



#Visualize With And Without Outliers
data_under_num %>%
  plot_outlier(diagnose_outlier(data_under_num) %>%
                 filter(outliers_ratio >= 0.5) %>%          # dplyr
                 select(variables) %>%
                 unlist())


# Define Outlier functions
#Cap
outlier_cap <- function(x){
  quantiles <- quantile(x, c(.05, .95))
  x[x < quantiles[1]] <- quantiles[1]
  x[x > quantiles[2]] <- quantiles[2]
  x
}

#Remove
outlier_remove <- function(x){
  quantiles <- quantile(x, c(.05, .95))
  x[x < quantiles[1]] <- NA
  x[x > quantiles[2]] <- NA
  x
}

# Apply outlier function to data set
data_under_num_removed <- map_df(data_under_num, outlier_remove)
data_under_num_removed <- data_under_num_removed[complete.cases(data_under_num_removed),]


boxplot(scale(data_under_num_removed), xaxt = "n")
text(x = 1:length(data_under_num_removed),
     y = par("usr")[3] - 0.8,
     labels = names(data_under_num_removed),
     xpd = NA,
     ## Rotate the labels by 35 degrees.
     srt = 35,
     cex = 0.8,
     adj = 1)

# New data set without outliers
data_under_removed <- cbind(data_under_num, data_under_cat)

#Looks good !

#impute only single cols
diagnose_numeric(data_under_num)
imp_income <- imputate_outlier(data_under, annual_inc, method = "median")
imp_income
summary(imp_income)
```



##Choose the appropriate visualization to investigate the distribution of the numeric features per the two levels of our target feature (i.e. default vs non-default).
```{r}

```

##Use a bar plot visualization to investigate the associations between the categorical variables and the target feature.
```{r}

```

##Visualize the correlations that emerge between the numerical features.
```{r}
corr <- cor(clean_num)
ggcorrplot(corr)
corrplot(corr)
p_value_mat <- cor_pmat(clean_num)
ggcorrplot(corr, type = "lower", p.mat = p_value_mat) 
```

##Plot an interactive scatter plot of the association between the loan amount requested and the annual income of the borrower.
```{r}

```

##Create a new balanced data set where the two levels of the target variable will be equally represented; Create a bar plot of the newly created target variable.
```{r}
set.seed(7)
data_balanced <- ovun.sample(Status ~ ., data=data, method = "under")
data_under <- data.frame(data_balanced[["data"]])

table(data_under$Status)
barplot(table(data_under$Status))
```

#Exercise 2
##Train and test a logistic classifier. Specifically
###Divide the sample into training and testing set using 80% for training the algorithm.
```{r}
set.seed(7)
div <- createDataPartition(y = features$Status, p = 0.8, list = F)

data.train <- features[div,]

data.test <- features[-div,]
```

###Train the classifier and report the coefficients obtained and interpret the results.
```{r}
fit1 <- glm(Status ~ ., data=data.train,family=binomial())
summary(fit1)
```

###Plot the ROC and the Precision/Recall Curve and interpret the results.
```{r}
#ROC
data.test$fit1_score <- predict(fit1,type='response',data.test)
fit1_pred <- prediction(data.test$fit1_score, data.test$Status)
fit1_roc <- performance(fit1_pred, "tpr", "fpr")
plot(fit1_roc, lwd=1, colorize = TRUE, main = "Fit1: Logit - ROC Curve")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1, lty=3)

#precision/ recall curve
fit1_precision <- performance(fit1_pred, measure = "prec", x.measure = "rec")
plot(fit1_precision, main="Fit1: Logit - Precision vs Recall")
```

###Produce the confusion matrix and interpret the results.
```{r}
confusionMatrix(as.factor(round(data.test$fit1_score)), data.test$Status)
```

###Report the AUC values and the overall accuracy and interpret the results.
```{r}
fit1_auc <- performance(fit1_pred, measure = "auc")
cat("AUC: ",fit1_auc@y.values[[1]]*100)
```

#Exercise 3
##Can you think of a way to improve the predictive performance of your data?
There are several possible ways to improve the predictive performance of our data. For example the handling of outliers as the way dealt with outliers can have a huge outcome on how the predictive performance of the affected data. Furthermore the usage of more training data will most certainly improve the performance of our mode.

##What can you do differently? (hint: Feel free to be creative and discuss any additional step in data collection and/or data pre-processing that you might try so to improve the results)
The collection of additional Data to provide more training data could be a way to improve our results. In addition to that we could change/play around the way dealt with outliers in our process to test wheter or not the results would improve.

#Exercise 4
##What kind of challenges may a company face if it would use your model in their daily business, in particular in regard to ethical challenges and moral obligations companies have?
The model makes its decisions based on the data provided. As the model decides whether or not a loan contract is defaulted or not based on the data provided, this can become a challenge as the used data may not be accurate or big enough to train the model well enough to be able to make a fair decision if a loan contract is defaulted or not.
In addition to that it is questionable if such a decision should be made by a model itself as there may be information that can not be considered by the model.

##Can you think of a way how companies can overcome or at least mitigate the issues that you described above?
A possible way to mitigate such challenges could for example be to consider if all information needed to make such a decision is available in the data used for a model. Additionally it should be assessed whether or not the decision made by the model should be absoulte or act as a supportive indicator for the company.